{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from models import build_model\n",
    "from engine import evaluate, train_one_epoch\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Device: cuda\n"
     ]
    }
   ],
   "source": [
    "## define my device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Available Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets analyze simplified DETR\n",
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                    num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        print(f\"Backbone Pre-Output Shape: {x.shape}\")\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        print(f\"Backbone Post-Output Shape: {h.shape}\")\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        print(f\"H: {H} and W: {W}\")\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "        print(f\"Positional Embeddings Shape: {pos.shape}\")\n",
    "        print(f\"Transformer Input Tensor Shape: {h.flatten(2).permute(2, 0, 1).shape}\")\n",
    "\n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                                self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initialize the model\n",
    "detr = DETRdemo(num_classes=91)\n",
    "state_dict = torch.hub.load_state_dict_from_url(\n",
    "    url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "    map_location='cpu', check_hash=True)\n",
    "detr.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone Pre-Output Shape: torch.Size([1, 2048, 7, 7])\n",
      "Backbone Post-Output Shape: torch.Size([1, 256, 7, 7])\n",
      "H: 7 and W: 7\n",
      "Positional Embeddings Shape: torch.Size([49, 1, 256])\n",
      "Transformer Input Tensor Shape: torch.Size([49, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "## create dummy input\n",
    "dummy_input = torch.ones((1, 3, 224, 224))\n",
    "dummy_out = detr(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\citak/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "## load the dino model\n",
    "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_dummy_output = dino_model(dummy_input)\n",
    "dino_dummy_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1024])\n",
      "torch.Size([1, 1024, 256])\n",
      "torch.Size([1, 1024, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "dino_dummy_output_norm_patches = dino_model.forward_features(dummy_input)[\"x_norm_patchtokens\"]\n",
    "print(dino_dummy_output_norm_patches.shape)\n",
    "\n",
    "dino_dummy_output_norm_patches = dino_dummy_output_norm_patches.permute(0, 2, 1)\n",
    "print(dino_dummy_output_norm_patches.shape)\n",
    "\n",
    "dino_dummy_output_norm_patches = dino_dummy_output_norm_patches.reshape(1, 1024, int(224/14), int(224/14))\n",
    "print(dino_dummy_output_norm_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_dummy_output_pathches = dino_model.get_intermediate_layers(dummy_input, n=1)[0]\n",
    "dino_dummy_output_pathches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape to 16x16 grid, then downsample to 7x7\n",
    "B, N, D = dino_dummy_output_pathches.shape\n",
    "B, N, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16, 1024])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_features = dino_dummy_output_pathches.view(B, 16, 16, D)\n",
    "dino_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_features.permute(0, 3, 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 1024])\n"
     ]
    }
   ],
   "source": [
    "dino_features = F.adaptive_avg_pool2d(dino_features.permute(0, 3, 1, 2), (7, 7))\n",
    "dino_features = dino_features.flatten(2).permute(0, 2, 1)  # Shape: [B, 49, 1024]\n",
    "\n",
    "print(dino_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Project 1024-dim to 256-dim using Linear layer\n",
    "linear_proj = nn.Linear(1024, 256)\n",
    "features_transformed = linear_proj(dino_features)  # Shape: [1, 49, 256] but DETR expects: Transformer Input Tensor Shape: torch.Size([49, 1, 256])\n",
    "features_transformed = features_transformed.permute(1,0,2)\n",
    "print(features_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1024])\n",
      "torch.Size([1, 255, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(dino_dummy_output_pathches.shape)\n",
    "patch_features = dino_dummy_output_pathches[:, 1:, :] # remove the CLS token\n",
    "print(patch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16, 1024])\n"
     ]
    }
   ],
   "source": [
    "def unflatten_features(features, batch_size=1, patch_size=14, img_size=224):\n",
    "    return features.reshape(batch_size, img_size // patch_size, img_size // patch_size, -1)\n",
    "\n",
    "unflatten_elements = unflatten_features(dino_dummy_output_pathches, patch_size=14)\n",
    "print(unflatten_elements.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 7, 7, 1024])\n"
     ]
    }
   ],
   "source": [
    "class LearnableDownsample(nn.Module):\n",
    "    def __init__(self, in_channels=1024, out_channels=1024):\n",
    "        super(LearnableDownsample, self).__init__()\n",
    "        # First convolution layer to reduce 16x16 to 8x8\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        # Second convolution layer to reduce 8x8 to 7x7\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reorder to [1, 1024, 16, 16]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = self.conv1(x)  # Shape: [1, 1024, 8, 8]\n",
    "        x = self.conv2(x)  # Shape: [1, 1024, 7, 7]\n",
    "        \n",
    "        # Reorder back to [1, 7, 7, 1024]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(1, 16, 16, 1024)  # Input tensor of shape [1, 16, 16, 1024]\n",
    "downsample_model = LearnableDownsample()\n",
    "output = downsample_model(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [1, 7, 7, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 49, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PoolingAndProjection(nn.Module):\n",
    "    def __init__(self, input_dim=1024, num_patches=49, output_dim=256):\n",
    "        super(PoolingAndProjection, self).__init__()\n",
    "        # Adaptive pooling to reduce the feature map from 256 to 49 patches\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # Reduces to a 7x7 grid, hence 49 patches\n",
    "        self.linear_proj = nn.Linear(input_dim, output_dim)  # Linear projection to reduce features to 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [256, 1, 1024]\n",
    "        x = x.squeeze(1)  # Shape: [256, 1024]\n",
    "        \n",
    "        # Reshape to [16, 16, 1024] (simulating the spatial structure of patches)\n",
    "        x = x.view(16, 16, 1024).permute(2, 0, 1)  # Shape: [1024, 16, 16]\n",
    "        \n",
    "        # Apply average pooling to reduce spatial dimensions to 7x7 grid\n",
    "        pooled_features = self.pool(x.unsqueeze(0))  # Shape: [1, 1024, 7, 7]\n",
    "        \n",
    "        # Flatten to 49 patches\n",
    "        pooled_features = pooled_features.flatten(2).permute(0, 2, 1)  # Shape: [1, 49, 1024]\n",
    "        \n",
    "        # Apply linear projection to reduce to 256 features\n",
    "        output = self.linear_proj(pooled_features)  # Shape: [1, 49, 256]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "dino_features = torch.randn(256, 1, 1024)  # Example input of shape [256, 1, 1024]\n",
    "pooling_projection_model = PoolingAndProjection()\n",
    "output = pooling_projection_model(dino_features)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [1, 49, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dino object detection: https://github.com/facebookresearch/dinov2/issues/350\n",
    "## https://zburkett.io/ai/2023/09/24/pca-object-detection.html\n",
    "## https://github.com/itsprakhar/Yolo-DinoV2/blob/bd05c8b0afabfa39cb2cd7b1d915093c3963c37b/ultralytics/nn/modules/pretrained_vit.py#L15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
