{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import types\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "import torchvision as TV\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from models import build_model\n",
    "from engine import evaluate, train_one_epoch\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define my device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Available Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation with multi-batch support.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Supports multiple batch sizes.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, num_queries=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # create dinov2 backbone and then\n",
    "        # arrange the other projections\n",
    "        self.dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.dino_projector = nn.Linear(1024, 256)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.num_queries = num_queries\n",
    "        self.query_pos = nn.Parameter(torch.rand(num_queries, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "\n",
    "        # Freeze all layers by setting requires_grad=False for all parameters\n",
    "        for param in self.dino_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(\"The Dino model is now frozen and will not require gradients during training.\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through DinoV2\n",
    "        x = self.dino_model.forward_features(inputs)[\"x_norm_patchtokens\"]\n",
    "        img_B, img_C, img_W, img_H = inputs.shape\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # Reshape and process backbone features\n",
    "        x = x.view(B, int(img_W / 14), int(img_H / 14), D)\n",
    "        x = self.avg_pool(x.permute(0, 3, 1, 2))\n",
    "        x = x.flatten(2).permute(0, 2, 1)\n",
    "        x = self.dino_projector(x)  # Shape: [B, 49, 256]\n",
    "        h = x.permute(1, 0, 2)  # Shape: [49, B, 256]\n",
    "\n",
    "        # Construct positional encodings\n",
    "        H, W = 7, 7\n",
    "        \n",
    "        # Modify positional encoding to support batch size\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Repeat query_pos for batch size\n",
    "        query_pos = self.query_pos.unsqueeze(1).repeat(1, B, 1)\n",
    "\n",
    "        # Propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h, query_pos).transpose(0, 1)\n",
    "        \n",
    "        # Project transformer outputs to class labels and bounding boxes\n",
    "        return {\n",
    "            'pred_logits': self.linear_class(h), \n",
    "            'pred_boxes': self.linear_bbox(h).sigmoid()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "bddk_num_classes = 7\n",
    "model = DETRdemo(num_classes= bddk_num_classes).to(device)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "state_dict = torch.hub.load_state_dict_from_url(\n",
    "    url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "    map_location='cpu', check_hash=True)\n",
    "\n",
    "# Since we will fine-tune for different number of classes (COCO:91 v Our Problem: 7)\n",
    "layers_to_remove = [\"linear_class.weight\", \"linear_class.bias\", \"linear_bbox.weight\", \"linear_bbox.bias\"]\n",
    "\n",
    "for layer in layers_to_remove:\n",
    "    if layer in state_dict:\n",
    "        del state_dict[layer]\n",
    "\n",
    "\n",
    "# 1. Get model and pretrained state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "pretrained_state_dict = state_dict  # Assuming it's already loaded\n",
    "\n",
    "# 2. Find common keys\n",
    "matching_keys = model_state_dict.keys() & pretrained_state_dict.keys()\n",
    "\n",
    "# 3. Create a filtered state_dict with only the matching layers\n",
    "filtered_state_dict = {key: pretrained_state_dict[key] for key in matching_keys}\n",
    "\n",
    "# 4. Load the filtered state_dict into your model\n",
    "missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "print(f'# of Missing: {len(missing_keys)} and Matched: {len(filtered_state_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you uncomment and run, you will see the missing keys are related to;\n",
    "## 1) Dino model\n",
    "## 2) Last layer that works for classification and bboxes\n",
    "\"\"\"\n",
    "for element in missing_keys:\n",
    "    print(element)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "dummy_inputs = torch.randn(4, 3, 448, 448).to(device)\n",
    "\n",
    "# Forward pass\n",
    "dummy_outputs = model(dummy_inputs)\n",
    "\n",
    "print(\"Pred Logits Shape:\", dummy_outputs['pred_logits'].shape)\n",
    "print(\"Pred Boxes Shape:\", dummy_outputs['pred_boxes'].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input with transformation\n",
    "dummy_inputs = np.random.randint(0, 256, (448, 448, 3), dtype=np.uint8)\n",
    "dummy_inputs = PIL.Image.fromarray(dummy_inputs)\n",
    "\n",
    "normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "dummy_transformation = T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.Resize((448, 448)),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "\n",
    "dummy_transformed = dummy_transformation(dummy_inputs).to(device)\n",
    "dummy_transformed = dummy_transformed.unsqueeze(0)\n",
    "\n",
    "print(dummy_transformed.shape)\n",
    "\n",
    "dummy_transformed_duplicated = dummy_transformed.repeat(5, 1, 1, 1)\n",
    "\n",
    "print(dummy_transformed_duplicated.shape)\n",
    "\n",
    "# Forward pass\n",
    "dummy_outputs = model(dummy_transformed_duplicated)\n",
    "\n",
    "print(\"Pred Logits Shape:\", dummy_outputs['pred_logits'].shape)\n",
    "print(\"Pred Boxes Shape:\", dummy_outputs['pred_boxes'].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import detr as detr_models\n",
    "from models import matcher as detr_matcher\n",
    "\n",
    "from datasets.coco import CocoDetection as coco_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of loss function and post-processor\n",
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 1, 'loss_giou': 1}\n",
    "weight_dict = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in weight_dict.items()}\n",
    "\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "\n",
    "eos_coef = 0.1\n",
    "eos_coef = torch.tensor(eos_coef, dtype=torch.float32, device=device)\n",
    "\n",
    "bddk_matcher = detr_matcher.HungarianMatcher(cost_class=1, cost_bbox=5, cost_giou=2)\n",
    "\n",
    "criterion = detr_models.SetCriterion(bddk_num_classes, matcher=bddk_matcher, weight_dict=weight_dict, eos_coef=eos_coef, losses=losses, device=device)\n",
    "postprocessors = {'bbox': detr_models.PostProcess()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of optimizer\n",
    "lr = 5e-5\n",
    "weight_decay = 1e-4\n",
    "lr_drop = 2\n",
    "\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_dicts, lr=lr,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of dataset and transformation function\n",
    "\n",
    "def dino_transforms(image_set):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    if image_set == 'train':\n",
    "        \n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.Resize((448, 448)),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return T.Compose([\n",
    "            T.Resize((448, 448)),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "images_main_path = r'C:\\BDD100K\\100k_images'\n",
    "train_images_path = os.path.join(images_main_path, 'train')\n",
    "val_images_path = os.path.join(images_main_path, 'val')\n",
    "test_images_path = os.path.join(images_main_path, 'test')\n",
    "\n",
    "print('-*'*20)\n",
    "dataset_train = coco_dataset(train_images_path, '../train_subset_COCO_Format.json', transforms=dino_transforms('train'), return_masks=False)\n",
    "print('-*'*20)\n",
    "dataset_val = coco_dataset(val_images_path, '../val_subset_COCO_Format.json', transforms=dino_transforms('val'), return_masks=False)\n",
    "print('-*'*20)\n",
    "dataset_test = coco_dataset(test_images_path, '../test_subset_COCO_Format.json', transforms=dino_transforms('val'), return_masks=False)\n",
    "print('-*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    \n",
    "    # Extract regular tensors instead of creating a NestedTensor\n",
    "    images = batch[0]\n",
    "    \n",
    "    # Get maximum dimensions to create a batch\n",
    "    max_size = utils._max_by_axis([list(img.shape) for img in images])\n",
    "    batch_shape = [len(images)] + max_size\n",
    "    b, c, h, w = batch_shape\n",
    "    \n",
    "    # Create padded tensor batch\n",
    "    dtype = images[0].dtype\n",
    "    device = images[0].device\n",
    "    tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "    \n",
    "    # Copy images to padded tensor\n",
    "    for i, img in enumerate(images):\n",
    "        tensor[i, :img.shape[0], :img.shape[1], :img.shape[2]].copy_(img)\n",
    "    \n",
    "    # Return tensor batch and the rest of the batch elements\n",
    "    batch[0] = tensor\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defintion of dataloader\n",
    "batch_size = 2\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn= my_collate_fn) # utils.collate_fn\n",
    "\n",
    "data_loader_val = DataLoader(dataset_val, batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn= my_collate_fn) # utils.collate_fn\n",
    "\n",
    "data_loader_test = DataLoader(dataset_test, batch_size, sampler=sampler_test,\n",
    "                                drop_last=False, collate_fn= my_collate_fn) # utils.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TRAINING (FINE-TUNING)\n",
    "start_epoch = 0\n",
    "n_epoch = 20\n",
    "\n",
    "clip_max_norm = 0.1\n",
    "clip_max_norm = torch.tensor(clip_max_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "output_dir = Path('weights')\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, n_epoch):\n",
    "\n",
    "    train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch, clip_max_norm)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "\n",
    "        # extra checkpoint before LR drop and every 2 epochs\n",
    "        if (epoch + 1) % lr_drop == 0 or (epoch + 1) % 2 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                     'epoch': epoch,\n",
    "                     'n_parameters': n_parameters}\n",
    "\n",
    "    if output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "        \n",
    "        # for evaluation logs\n",
    "        if coco_evaluator is not None:\n",
    "            (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "            if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                filenames = ['latest.pth']\n",
    "                if epoch % 1 == 0:\n",
    "                    filenames.append(f'{epoch:03}.pth')\n",
    "                for name in filenames:\n",
    "                    torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                output_dir / \"eval\" / name)\n",
    "    \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
